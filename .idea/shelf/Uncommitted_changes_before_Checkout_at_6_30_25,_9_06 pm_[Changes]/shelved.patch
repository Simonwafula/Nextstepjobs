Index: .env
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># .env\n# openai\nOPENAI_API_KEY = \"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\"\n\n# Aurora Db\n#PG_HOST=database-1-instance-1.crkskqske5wp.eu-north-1.rds.amazonaws.com\n#PG_PORT=5432\n#PG_DB=nextstepeduc\n#PG_USER=mstatili\n#PG_PASS=51mon_Waf+SuperSecretPassword\n
===================================================================
diff --git a/.env b/.env
--- a/.env	(revision 149515dafb2fe9d1a83f1abe1d94719b1f8c707a)
+++ b/.env	(date 1751306667543)
@@ -8,3 +8,24 @@
 #PG_DB=nextstepeduc
 #PG_USER=mstatili
 #PG_PASS=51mon_Waf+SuperSecretPassword
+
+# Database Configuration
+INPUT_DB_PATH=db/jobs.sqlite3
+OUTPUT_DB_PATH=db/processed_jobs.sqlite3
+
+# Processing Configuration
+LLM_MODEL=gpt-4o-mini
+TEMPERATURE=0.1
+BATCH_SIZE=10
+MAX_RETRIES=3
+MAX_CONCURRENT=5
+
+# Logging Configuration
+LOG_LEVEL=INFO
+LOG_FILE=pipeline.log
+
+# Career Insights Configuration
+EXPORT_FILE=career_insights.json
+
+# Optional: Rate limiting (requests per minute)
+API_RATE_LIMIT=60
\ No newline at end of file
Index: processors/pipeline.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># processors/pipeline.py\nimport warnings\n\nfrom urllib3.exceptions import NotOpenSSLWarning\n\n# Suppress only the LibreSSL/OpenSSL compatibility warning before any urllib3 imports\nwarnings.filterwarnings(\n    \"ignore\",\n    category=NotOpenSSLWarning,\n    module=\"urllib3\"\n)\n\nimport os\nimport json\nimport sqlite3\nimport logging\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom typing import Optional, List, Tuple\n\nfrom dotenv import load_dotenv\nfrom langchain_core.exceptions import OutputParserException\n\n# Load environment variables from .env file\nload_dotenv()\n\nfrom langchain.chains.llm import LLMChain\nfrom langchain_core.output_parsers import PydanticOutputParser\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_openai import OpenAI\nfrom processors.models import JobStructured\n\n# Logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)-8s %(message)s\")\n\n# Environment & DB settings\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nif not OPENAI_API_KEY:\n    raise RuntimeError(\"Please set OPENAI_API_KEY environment variable.\")\n\nRAW_DB       = os.getenv(\"JOB_DB_PATH\", \"db/jobs.sqlite3\")\nRAW_TABLE    = os.getenv(\"RAW_TABLE\", \"jobs_data\")\nSTRUCT_DB    = os.getenv(\"STRUCT_DB_PATH\", \"db/jobs_structured.sqlite3\")\nSTRUCT_TABLE = os.getenv(\"STRUCT_TABLE\", \"jobs_structured\")\n\n# Initialize structured DB\ndef ensure_structured_db():\n    conn = sqlite3.connect(STRUCT_DB)\n    conn.execute(\n        f\"CREATE TABLE IF NOT EXISTS {STRUCT_TABLE} (id INTEGER PRIMARY KEY, json TEXT)\"\n    )\n    conn.commit()\n    conn.close()\n    logging.info(\"Structured database initialized.\")\n\n# Fetch raw records\ndef fetch_raw(batch_size: int) -> List[Tuple[int, str]]:\n    conn = sqlite3.connect(RAW_DB)\n    cur  = conn.cursor()\n    cur.execute(f\"SELECT id, content FROM {RAW_TABLE} LIMIT ?\", (batch_size,))\n    rows = cur.fetchall()\n    conn.close()\n    return rows\n\n# Persist structured results\ndef persist_structured(records: List[Tuple[int, str]]):\n    conn = sqlite3.connect(STRUCT_DB)\n    cur  = conn.cursor()\n    cur.executemany(\n        f\"INSERT OR REPLACE INTO {STRUCT_TABLE} (id, json) VALUES (?,?)\",\n        records\n    )\n    conn.commit()\n    conn.close()\n    logging.info(f\"Saved {len(records)} structured records.\")\n\n# Setup LangChain with format instructions\ndef create_chain():\n    # Initialize Pydantic parser for schema enforcement\n    parser = PydanticOutputParser(pydantic_object=JobStructured)\n\n    # Base prompt template with placeholder for format instructions\n    prompt_template = PromptTemplate(\n        input_variables=[\"description\", \"format_instructions\"],\n        template=\"\"\"\nYou are an expert at extracting structured JSON from job description text.\nReturn exactly the JSON matching the JobStructured schema.\n\nJob Description:\n{description}\n\n{\n  \"id\": int,\n  \"company_name\": string,\n  \"location\": string,\n  \"post_date\": string (YYYY-MM-DD or empty),\n  \"education_requirements\": [\n    {\n      \"level\": \"diploma|certificate|associate|bachelor|master|phd|professional_license|none_specified\",\n      \"field\": string,\n      \"requirement_type\": \"required|preferred|equivalent_experience_accepted\",\n      \"years_experience_in_lieu\": number or null\n    }\n  ],\n  \"job_classification\": { /* follow JobClassification model */ },\n  \"location_and_work\": { /* follow LocationWork model */ },\n  \"skills\": { /* follow SkillsTaxonomy model */ },\n  \"certifications\": [ /* list of Certification objects */ ],\n  \"career_progression\": { /* follow CareerProgression model */ },\n  \"compensation\": { /* follow CompensationBenefits model */ },\n  \"market_intelligence\": { /* follow MarketIntelligence model */ },\n  \"work_environment\": { /* follow WorkEnvironment model */ }\n}\n\n{format_instructions}\n\"\"\"\n    )\n\n    # Inject format instructions so chain only needs 'description'\n    prompt = prompt_template.partial(\n        format_instructions=parser.get_format_instructions()\n    )\n\n    llm = OpenAI(temperature=0)\n    chain = LLMChain(\n        llm=llm,\n        prompt=prompt\n    )\n    return chain, parser\n\n# Process a single record with validation with validation\ndef process_record(record: Tuple[int, str], chain: LLMChain, parser: PydanticOutputParser) -> Optional[Tuple[int, str]]:\n    job_id, text = record\n    try:\n        response = chain.invoke({\"description\": text})\n        # Parse and validate\n        structured: JobStructured = parser.parse(response)\n        data = structured.dict()\n        return (job_id, json.dumps(data))\n    except OutputParserException as e:\n        logging.error(f\"Parsing failed id={job_id}: {e}\")\n    except Exception as e:\n        logging.error(f\"Failed to process id={job_id}: {e}\")\n    return None\n\n# Main batch processing\ndef main(batch: int = 10, workers: int = 5):\n    ensure_structured_db()\n    raw_records = fetch_raw(batch)\n    if not raw_records:\n        logging.info(\"No new records to process.\")\n        return\n\n    chain, parser = create_chain()\n    results = []\n    with ThreadPoolExecutor(max_workers=workers) as executor:\n        futures = [executor.submit(process_record, r, chain, parser) for r in raw_records]\n        for fut in as_completed(futures):\n            res = fut.result()\n            if res:\n                results.append(res)\n\n    persist_structured(results)\n\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser(description=\"Process raw job descriptions to structured JSON.\")\n    parser.add_argument(\"--batch\", type=int, default=10)\n    parser.add_argument(\"--workers\", type=int, default=5)\n    args = parser.parse_args()\n    main(batch=args.batch, workers=args.workers)\n
===================================================================
diff --git a/processors/pipeline.py b/processors/pipeline.py
--- a/processors/pipeline.py	(revision 149515dafb2fe9d1a83f1abe1d94719b1f8c707a)
+++ b/processors/pipeline.py	(date 1751306692884)
@@ -1,170 +1,1247 @@
-# processors/pipeline.py
-import warnings
-
-from urllib3.exceptions import NotOpenSSLWarning
-
-# Suppress only the LibreSSL/OpenSSL compatibility warning before any urllib3 imports
-warnings.filterwarnings(
-    "ignore",
-    category=NotOpenSSLWarning,
-    module="urllib3"
-)
-
 import os
-import json
+import re
 import sqlite3
 import logging
-from concurrent.futures import ThreadPoolExecutor, as_completed
-from typing import Optional, List, Tuple
+import asyncio
+import json
+from typing import List, Optional, Literal, Dict, Any
+from datetime import datetime, time
 
 from dotenv import load_dotenv
-from langchain_core.exceptions import OutputParserException
-
-# Load environment variables from .env file
-load_dotenv()
-
-from langchain.chains.llm import LLMChain
-from langchain_core.output_parsers import PydanticOutputParser
 from langchain_core.prompts import PromptTemplate
 from langchain_openai import OpenAI
-from processors.models import JobStructured
+from langchain_core.output_parsers import PydanticOutputParser
+from langchain_core.runnables import RunnableSequence
+from pydantic import BaseModel, Field, validator
+
+# Load environment variables
+load_dotenv()
+
+# Regex patterns for abbreviation normalization
+BS_RE = re.compile(r"\bB\.S\.\b", re.IGNORECASE)
+BA_RE = re.compile(r"\bB\.A\.\b", re.IGNORECASE)
+MS_RE = re.compile(r"\bM\.S\.\b", re.IGNORECASE)
+MA_RE = re.compile(r"\bM\.A\.\b", re.IGNORECASE)
+PHD_RE = re.compile(r"\bPh\.D\.\b", re.IGNORECASE)
+WHITESPACE_RE = re.compile(r"\s+")
+URL_RE = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
+
+# Configure logging
+logging.basicConfig(
+    level=logging.INFO,
+    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
+    handlers=[
+        logging.FileHandler('pipeline.log'),
+        logging.StreamHandler()
+    ]
+)
+logger = logging.getLogger(__name__)
+
+
+# Enhanced data models with validation
+class JobClassification(BaseModel):
+    category: Optional[str] = None
+    level: Optional[str] = None
+    function: Optional[str] = None
+    department: Optional[str] = None
+
+    @validator('level')
+    def validate_level(cls, v):
+        if v:
+            valid_levels = ['entry', 'junior', 'mid', 'senior', 'lead', 'manager', 'director', 'executive']
+            v_lower = v.lower()
+            for level in valid_levels:
+                if level in v_lower:
+                    return level.title()
+        return v
+
+
+class LocationWork(BaseModel):
+    office_location: Optional[str] = None
+    remote: Optional[bool] = None
+    onsite: Optional[bool] = None
+    hybrid: Optional[bool] = None
+    travel_required: Optional[str] = None
+
+    @validator('office_location')
+    def clean_location(cls, v):
+        if v:
+            return v.strip().title()
+        return v
+
+
+class SkillsTaxonomy(BaseModel):
+    main_skill: Optional[str] = None
+    technical_skills: List[str] = Field(default_factory=list)
+    soft_skills: List[str] = Field(default_factory=list)
+    tools_technologies: List[str] = Field(default_factory=list)
+    programming_languages: List[str] = Field(default_factory=list)
+    frameworks: List[str] = Field(default_factory=list)
+
+    @validator('technical_skills', 'soft_skills', 'tools_technologies',
+               'programming_languages', 'frameworks', pre=True)
+    def clean_skills_list(cls, v):
+        if isinstance(v, str):
+            return [skill.strip() for skill in v.split(',') if skill.strip()]
+        elif isinstance(v, list):
+            return [skill.strip() for skill in v if skill and skill.strip()]
+        return []
+
+
+class Certification(BaseModel):
+    name: Optional[str] = None
+    issuer: Optional[str] = None
+    year: Optional[int] = None
+    required: Optional[bool] = False
+
+    @validator('year')
+    def validate_year(cls, v):
+        if v and (v < 1950 or v > datetime.now().year + 5):
+            return None
+        return v
+
+
+class CareerProgression(BaseModel):
+    entry_level: Optional[str] = None
+    mid_level: Optional[str] = None
+    senior_level: Optional[str] = None
+    growth_opportunities: List[str] = Field(default_factory=list)
+
+
+class CompensationBenefits(BaseModel):
+    salary_min: Optional[float] = None
+    salary_max: Optional[float] = None
+    currency: Optional[str] = "USD"
+    salary_type: Optional[str] = None  # hourly, annual, contract
+    benefits: List[str] = Field(default_factory=list)
+
+    @validator('salary_min', 'salary_max')
+    def validate_salary(cls, v):
+        if v and (v < 0 or v > 10000000):  # Reasonable bounds for salary
+            return None
+        return v
+
+
+class EducationRequirement(BaseModel):
+    level: Literal[
+        "high_school", "certificate", "diploma", "associate",
+        "bachelor", "master", "phd", "professional_license",
+        "none_specified", "equivalent_experience"
+    ]
+    field: Optional[str] = None
+    requirement_type: Literal["required", "preferred", "equivalent_experience_accepted"]
+    years_experience_substitute: Optional[int] = None
+    confidence_score: float = Field(ge=0.0, le=1.0)
+
+    @validator('field')
+    def clean_field(cls, v):
+        if v:
+            return v.lower().strip()
+        return v
+
+
+class EducationExtraction(BaseModel):
+    requirements: List[EducationRequirement]
+    raw_text_analyzed: str
+
+
+class JobExtraction(BaseModel):
+    # Basic fields with validation
+    title_clean: Optional[str] = None
+    company: Optional[str] = None
+    company_location: Optional[str] = None
+    post_date: Optional[str] = None
+    industry: Optional[str] = None
+    job_type: Optional[str] = None
+    job_category: Optional[str] = None
+    job_description: Optional[str] = None
+    application_deadline: Optional[str] = None
+    additional_requirements: Optional[str] = None
+    full_link: Optional[str] = None
+    experience_required: Optional[str] = None
+    company_size: Optional[str] = None
+
+    # Nested structures with defaults
+    job_classification: JobClassification = Field(default_factory=JobClassification)
+    location_and_work: LocationWork = Field(default_factory=LocationWork)
+    skills: SkillsTaxonomy = Field(default_factory=SkillsTaxonomy)
+    certifications: List[Certification] = Field(default_factory=list)
+    career_progression: CareerProgression = Field(default_factory=CareerProgression)
+    compensation: CompensationBenefits = Field(default_factory=CompensationBenefits)
+    education_requirements: List[EducationRequirement] = Field(default_factory=list)
+
+    raw_text_analyzed: str = Field(default="")
+    processing_timestamp: str = Field(default_factory=lambda: datetime.now().isoformat())
+
+    @validator('full_link')
+    def validate_url(cls, v):
+        if v and not URL_RE.match(v):
+            logger.warning(f"Invalid URL format: {v}")
+        return v
+
+    @validator('post_date', 'application_deadline')
+    def validate_dates(cls, v):
+        if v:
+            try:
+                # Try to parse common date formats
+                for fmt in ['%Y-%m-%d', '%m/%d/%Y', '%d/%m/%Y', '%B %d, %Y']:
+                    try:
+                        datetime.strptime(v, fmt)
+                        return v
+                    except ValueError:
+                        continue
+                logger.warning(f"Could not parse date: {v}")
+            except Exception:
+                pass
+        return v
+
+
+class AcademicDetailsProcessor:
+    def __init__(
+            self,
+            input_db_path: str = "db/jobs.sqlite3",
+            output_db_path: str = "db/processed_jobs.sqlite3",
+            llm_model: str = "gpt-4o-mini",
+            temperature: float = 0.1,
+            api_key: Optional[str] = None,
+            batch_size: int = 10,
+            max_retries: int = 3
+    ):
+        self.input_db_path = input_db_path
+        self.output_db_path = output_db_path
+        self.batch_size = batch_size
+        self.max_retries = max_retries
+
+        # Initialize API key
+        key = api_key or os.getenv("OPENAI_API_KEY")
+        if not key:
+            raise ValueError("OpenAI API key must be set via parameter or OPENAI_API_KEY env var")
+
+        # Initialize LLM and parsers
+        self.llm = OpenAI(model=llm_model, temperature=temperature, openai_api_key=key)
+        self.job_parser = PydanticOutputParser(pydantic_object=JobExtraction)
+        self.education_parser = PydanticOutputParser(pydantic_object=EducationExtraction)
+
+        # Create processing chains
+        self.job_chain = self._create_job_chain()
+        self.education_chain = self._create_education_chain()
+
+        # Setup database
+        self._setup_db()
+
+        logger.info(f"Processor initialized with model: {llm_model}")
+
+    def _create_job_chain(self) -> RunnableSequence:
+        """Create the job extraction chain"""
+        prompt = PromptTemplate.from_template(
+            """Extract comprehensive job information from the following job posting.
+
+            Focus on extracting:
+            1. Basic job details (title, company, location, dates)
+            2. Job classification (category, level, function, department)
+            3. Work arrangement (remote/onsite/hybrid, location details)
+            4. Skills taxonomy (technical skills, soft skills, tools, programming languages, frameworks)
+            5. Certifications (name, issuer, whether required)
+            6. Career progression opportunities
+            7. Compensation and benefits
+            8. Experience requirements
+
+            Be thorough but accurate. If information is not clearly stated, use null/empty values.
+
+            Job Posting Text:
+            {text}
+
+            {format_instructions}"""
+        )
+        return prompt | self.llm | self.job_parser
+
+    def _create_education_chain(self) -> RunnableSequence:
+        """Create the education requirements extraction chain"""
+        prompt = PromptTemplate.from_template(
+            """Analyze the job posting and extract all education requirements.
+
+            For each education requirement, determine:
+            1. Education level (high_school, certificate, diploma, associate, bachelor, master, phd, professional_license, none_specified, equivalent_experience)
+            2. Field of study (if specified)
+            3. Whether it's required, preferred, or equivalent experience is accepted
+            4. How many years of experience can substitute for the education
+            5. Confidence score (0.0-1.0) for the extraction accuracy
+
+            Consider phrases like:
+            - "Bachelor's degree required" → required
+            - "Master's preferred" → preferred  
+            - "Degree or equivalent experience" → equivalent_experience_accepted
+            - "5 years experience in lieu of degree" → years_experience_substitute: 5
+
+            Job Posting Text:
+            {text}
+
+            {format_instructions}"""
+        )
+        return prompt | self.llm | self.education_parser
+
+    def _setup_db(self):
+        """Setup the output database with improved schema"""
+        conn = sqlite3.connect(self.output_db_path)
 
-# Logging
-logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)-8s %(message)s")
+        # Enable foreign keys
+        conn.execute("PRAGMA foreign_keys = ON")
 
-# Environment & DB settings
-OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
-if not OPENAI_API_KEY:
-    raise RuntimeError("Please set OPENAI_API_KEY environment variable.")
+        # Main jobs table with additional fields
+        conn.execute("""
+                     CREATE TABLE IF NOT EXISTS jobs_meta(
+                                                             job_id INTEGER PRIMARY KEY,
+                                                             full_link TEXT,
+                                                             title_clean TEXT,
+                                                             company TEXT,
+                                                             company_location TEXT,
+                                                             post_date TEXT,
+                                                             industry TEXT,
+                                                             job_type TEXT,
+                                                             job_category TEXT,
+                                                             job_description TEXT,
+                                                             application_deadline TEXT,
+                                                             additional_requirements TEXT,
+                                                             experience_required TEXT,
+                                                             company_size TEXT,
+                                                             processing_timestamp TEXT,
+                                                             created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
+                                                             updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
+                     )
+                     """)
 
-RAW_DB       = os.getenv("JOB_DB_PATH", "db/jobs.sqlite3")
-RAW_TABLE    = os.getenv("RAW_TABLE", "jobs_data")
-STRUCT_DB    = os.getenv("STRUCT_DB_PATH", "db/jobs_structured.sqlite3")
-STRUCT_TABLE = os.getenv("STRUCT_TABLE", "jobs_structured")
+        # Add indexes for better performance
+        conn.execute("CREATE INDEX IF NOT EXISTS idx_meta_job ON jobs_meta(job_id)")
+        conn.execute("CREATE INDEX IF NOT EXISTS idx_meta_company ON jobs_meta(company)")
+        conn.execute("CREATE INDEX IF NOT EXISTS idx_meta_industry ON jobs_meta(industry)")
 
-# Initialize structured DB
-def ensure_structured_db():
-    conn = sqlite3.connect(STRUCT_DB)
-    conn.execute(
-        f"CREATE TABLE IF NOT EXISTS {STRUCT_TABLE} (id INTEGER PRIMARY KEY, json TEXT)"
-    )
-    conn.commit()
-    conn.close()
-    logging.info("Structured database initialized.")
+        # Job classification table
+        conn.execute("""
+                     CREATE TABLE IF NOT EXISTS job_classification (
+                        job_id INTEGER PRIMARY KEY,
+                        category TEXT,
+                        level TEXT,
+                        function TEXT,
+                        department TEXT,
+                        FOREIGN KEY (job_id) REFERENCES jobs_meta (job_id) ON DELETE CASCADE
+                         )
+                     """)
+
+        # Location and work arrangement
+        conn.execute("""
+                     CREATE TABLE IF NOT EXISTS location_work (
+                         job_id INTEGER PRIMARY KEY,
+                         office_location TEXT,
+                         remote BOOLEAN,
+                         onsite BOOLEAN,
+                         hybrid BOOLEAN,
+                         travel_required TEXT,
+                         FOREIGN KEY(job_id) REFERENCES jobs_meta(job_id) ON DELETE CASCADE
+                         )
+                     """)
+
+        # Enhanced skills taxonomy
+        conn.execute("""
+                     CREATE TABLE IF NOT EXISTS skills_taxonomy (
+                         job_id INTEGER PRIMARY KEY,
+                         main_skill TEXT,
+                         technical_skills TEXT,
+                         soft_skills TEXT,
+                         tools_technologies TEXT,
+                         programming_languages TEXT,
+                         frameworks TEXT,
+                         FOREIGN KEY(job_id) REFERENCES jobs_meta(job_id) ON DELETE CASCADE
+                         )
+                     """)
+
+        # Certifications table
+        conn.execute("""
+                     CREATE TABLE IF NOT EXISTS certifications (
+                         id INTEGER PRIMARY KEY AUTOINCREMENT,
+                         job_id INTEGER,
+                         name TEXT,
+                         issuer TEXT,
+                         year INTEGER,
+                         required BOOLEAN DEFAULT FALSE,
+                         FOREIGN KEY(job_id) REFERENCES jobs_meta(job_id) ON DELETE CASCADE
+                         )
+                     """)
+
+        # Career progression
+        conn.execute("""
+                     CREATE TABLE IF NOT EXISTS career_progression (
+                         job_id INTEGER PRIMARY KEY,
+                         entry_level TEXT,
+                         mid_level TEXT,
+                         senior_level TEXT,
+                         growth_opportunities TEXT,
+                         FOREIGN KEY(job_id) REFERENCES jobs_meta(job_id) ON DELETE CASCADE
+                         )
+                     """)
+
+        # Compensation and benefits
+        conn.execute("""
+                     CREATE TABLE IF NOT EXISTS compensation (
+                         job_id INTEGER PRIMARY KEY,
+                         salary_min REAL,
+                         salary_max REAL,
+                         currency TEXT
+                         DEFAULT 'USD',
+                         salary_type TEXT,
+                         benefits TEXT,
+                         FOREIGN KEY(job_id) REFERENCES jobs_meta(job_id) ON DELETE CASCADE
+                         
+                     """)
+
+        # Education requirements table
+        conn.execute("""
+                     CREATE TABLE IF NOT EXISTS education_requirements (
+                         id INTEGER PRIMARY KEY AUTOINCREMENT,
+                         job_id INTEGER NOT NULL,
+                         level TEXT NOT NULL,
+                         field TEXT,
+                         requirement_type TEXT NOT NULL,
+                         years_experience_substitute INTEGER,
+                         confidence_score REAL NOT NULL,
+                         FOREIGN KEY(job_id) REFERENCES jobs_meta(job_id) ON DELETE CASCADE
+                     """)
+
+        # Processing status table for tracking
+        conn.execute("""
+                     CREATE TABLE IF NOT EXISTS processing_status (
+                        job_id INTEGER PRIMARY KEY,
+                        status TEXT DEFAULT 'pending',
+                         error_message TEXT,
+                         retry_count INTEGER DEFAULT 0,
+                         last_attempt TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
+                         FOREIGN KEY(job_id) REFERENCES jobs_meta(job_id) ON DELETE CASCADE
+                         )
+                     """)
+
+        # Add indexes for performance
+        conn.execute("CREATE INDEX IF NOT EXISTS idx_edu_job ON education_requirements(job_id)")
+        conn.execute("CREATE INDEX IF NOT EXISTS idx_cert_job ON certifications(job_id)")
+        conn.execute("CREATE INDEX IF NOT EXISTS idx_status ON processing_status(status)")
+
+        conn.commit()
+        conn.close()
+        logger.info("Database schema setup completed")
+
+    def _preprocess_text(self, text: str) -> str:
+        """Preprocess job posting text"""
+        if not text:
+            return ""
+
+        # Normalize whitespace
+        text = WHITESPACE_RE.sub(" ", text)
+
+        # Normalize degree abbreviations
+        for regex, replacement in [
+            (BS_RE, "Bachelor of Science"),
+            (BA_RE, "Bachelor of Arts"),
+            (MS_RE, "Master of Science"),
+            (MA_RE, "Master of Arts"),
+            (PHD_RE, "Doctor of Philosophy")
+        ]:
+            text = regex.sub(replacement, text)
+
+        return text.strip()
+
+    def _store_job_data(self, job_id: int, job_data: JobExtraction, education_data: EducationExtraction):
+        """Store extracted job data in database"""
+        conn = sqlite3.connect(self.output_db_path)
+
+        try:
+            conn.execute("BEGIN TRANSACTION")
+
+            # Store main job metadata
+            conn.execute("""
+                INSERT OR REPLACE INTO jobs_meta 
+                (job_id, full_link, title_clean, company, company_location, post_date, 
+                 industry, job_type, job_category, job_description, application_deadline, 
+                 additional_requirements, experience_required, company_size, processing_timestamp)
+                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
+            """, (
+                job_id, job_data.full_link, job_data.title_clean, job_data.company,
+                job_data.company_location, job_data.post_date, job_data.industry,
+                job_data.job_type, job_data.job_category, job_data.job_description,
+                job_data.application_deadline, job_data.additional_requirements,
+                job_data.experience_required, job_data.company_size, job_data.processing_timestamp
+            ))
+
+            # Store job classification
+            conn.execute("""
+                INSERT OR REPLACE INTO job_classification 
+                (job_id, category, level, function, department)
+                VALUES (?, ?, ?, ?, ?)
+            """, (
+                job_id, job_data.job_classification.category,
+                job_data.job_classification.level, job_data.job_classification.function,
+                job_data.job_classification.department
+            ))
+
+            # Store location and work details
+            conn.execute("""
+                INSERT OR REPLACE INTO location_work 
+                (job_id, office_location, remote, onsite, hybrid, travel_required)
+                VALUES (?, ?, ?, ?, ?, ?)
+            """, (
+                job_id, job_data.location_and_work.office_location,
+                job_data.location_and_work.remote, job_data.location_and_work.onsite,
+                job_data.location_and_work.hybrid, job_data.location_and_work.travel_required
+            ))
+
+            # Store skills taxonomy
+            conn.execute("""
+                INSERT OR REPLACE INTO skills_taxonomy 
+                (job_id, main_skill, technical_skills, soft_skills, tools_technologies, 
+                 programming_languages, frameworks)
+                VALUES (?, ?, ?, ?, ?, ?, ?)
+            """, (
+                job_id, job_data.skills.main_skill,
+                json.dumps(job_data.skills.technical_skills),
+                json.dumps(job_data.skills.soft_skills),
+                json.dumps(job_data.skills.tools_technologies),
+                json.dumps(job_data.skills.programming_languages),
+                json.dumps(job_data.skills.frameworks)
+            ))
 
-# Fetch raw records
-def fetch_raw(batch_size: int) -> List[Tuple[int, str]]:
-    conn = sqlite3.connect(RAW_DB)
-    cur  = conn.cursor()
-    cur.execute(f"SELECT id, content FROM {RAW_TABLE} LIMIT ?", (batch_size,))
-    rows = cur.fetchall()
-    conn.close()
-    return rows
+            # Clear and store certifications
+            conn.execute("DELETE FROM certifications WHERE job_id = ?", (job_id,))
+            for cert in job_data.certifications:
+                conn.execute("""
+                             INSERT INTO certifications (job_id, name, issuer, year, required)
+                             VALUES (?, ?, ?, ?, ?)
+                             """, (job_id, cert.name, cert.issuer, cert.year, cert.required))
+
+            # Store career progression
+            conn.execute("""
+                INSERT OR REPLACE INTO career_progression 
+                (job_id, entry_level, mid_level, senior_level, growth_opportunities)
+                VALUES (?, ?, ?, ?, ?)
+            """, (
+                job_id, job_data.career_progression.entry_level,
+                job_data.career_progression.mid_level, job_data.career_progression.senior_level,
+                json.dumps(job_data.career_progression.growth_opportunities)
+            ))
+
+            # Store compensation
+            conn.execute("""
+                INSERT OR REPLACE INTO compensation 
+                (job_id, salary_min, salary_max, currency, salary_type, benefits)
+                VALUES (?, ?, ?, ?, ?, ?)
+            """, (
+                job_id, job_data.compensation.salary_min, job_data.compensation.salary_max,
+                job_data.compensation.currency, job_data.compensation.salary_type,
+                json.dumps(job_data.compensation.benefits)
+            ))
+
+            # Clear and store education requirements
+            conn.execute("DELETE FROM education_requirements WHERE job_id = ?", (job_id,))
+            for req in education_data.requirements:
+                conn.execute("""
+                             INSERT INTO education_requirements
+                             (job_id, level, field, requirement_type, years_experience_substitute, confidence_score)
+                             VALUES (?, ?, ?, ?, ?, ?)
+                             """, (
+                                 job_id, req.level, req.field, req.requirement_type,
+                                 req.years_experience_substitute, req.confidence_score
+                             ))
+
+            # Update processing status
+            conn.execute("""
+                INSERT OR REPLACE INTO processing_status (job_id, status, last_attempt)
+                VALUES (?, 'completed', CURRENT_TIMESTAMP)
+            """, (job_id,))
+
+            conn.commit()
+            logger.info(f"Successfully stored data for job {job_id}")
+
+        except Exception as e:
+            conn.rollback()
+            logger.error(f"Failed to store data for job {job_id}: {e}")
+
+            # Update error status
+            try:
+                conn.execute("""
+                    INSERT OR REPLACE INTO processing_status 
+                    (job_id, status, error_message, retry_count, last_attempt)
+                    VALUES (?, 'error', ?, COALESCE((SELECT retry_count FROM processing_status WHERE job_id = ?), 0) + 1, CURRENT_TIMESTAMP)
+                """, (job_id, str(e), job_id))
+                conn.commit()
+            except:
+                pass
+
+            raise
+        finally:
+            conn.close()
+
+    def extract_and_store(self, job_id: int, full_link: str, content: str) -> tuple[JobExtraction, EducationExtraction]:
+        """Extract job information and education requirements, then store in database"""
+        processed_content = self._preprocess_text(content)
+
+        if not processed_content:
+            logger.warning(f"Job {job_id}: Empty content after preprocessing")
+            return JobExtraction(), EducationExtraction(requirements=[], raw_text_analyzed="")
+
+        retry_count = 0
+        while retry_count < self.max_retries:
+            try:
+                # Extract job information
+                job_data = self.job_chain.invoke({
+                    "text": processed_content,
+                    "format_instructions": self.job_parser.get_format_instructions()
+                })
+                job_data.full_link = full_link
+                job_data.raw_text_analyzed = content[:1000]  # Store first 1000 chars
+
+                # Extract education requirements
+                education_data = self.education_chain.invoke({
+                    "text": processed_content,
+                    "format_instructions": self.education_parser.get_format_instructions()
+                })
+                education_data.raw_text_analyzed = content[:500]
+
+                # Store in database
+                self._store_job_data(job_id, job_data, education_data)
+
+                logger.info(
+                    f"Job {job_id}: Successfully processed with {len(education_data.requirements)} education requirements")
+                return job_data, education_data
+
+            except Exception as e:
+                retry_count += 1
+                logger.error(f"Job {job_id}: Attempt {retry_count} failed: {e}")
+
+                if retry_count >= self.max_retries:
+                    logger.error(f"Job {job_id}: Max retries exceeded")
+                    # Return empty structures on final failure
+                    empty_job = JobExtraction(
+                        full_link=full_link,
+                        raw_text_analyzed=content[:1000] if content else ""
+                    )
+                    empty_education = EducationExtraction(
+                        requirements=[],
+                        raw_text_analyzed=content[:500] if content else ""
+                    )
+                    return empty_job, empty_education
+
+                # Wait before retry
+                time.sleep(2 ** retry_count)  # Exponential backoff
+
+    def get_unprocessed_jobs(self) -> List[tuple]:
+        """Get jobs that haven't been processed yet or failed processing"""
+        input_conn = sqlite3.connect(self.input_db_path)
+        output_conn = sqlite3.connect(self.output_db_path)
+
+        # Get jobs that are not in the processed database or have failed
+        query = """
+                SELECT jd.id, jd.full_link, jd.content
+                FROM jobs_data jd
+                         LEFT JOIN processing_status ps ON jd.id = ps.job_id
+                WHERE ps.job_id IS NULL
+                   OR ps.status = 'error'
+                   OR (ps.status = 'pending' AND ps.retry_count < ?)
+                ORDER BY jd.id \
+                """
+
+        try:
+            rows = input_conn.execute(query, (self.max_retries,)).fetchall()
+            logger.info(f"Found {len(rows)} jobs to process")
+            return rows
+        except sqlite3.Error as e:
+            logger.error(f"Error querying unprocessed jobs: {e}")
+            return []
+        finally:
+            input_conn.close()
+            output_conn.close()
+
+    def batch_extract(self) -> List[tuple[JobExtraction, EducationExtraction]]:
+        """Process jobs in batches"""
+        unprocessed_jobs = self.get_unprocessed_jobs()
+
+        if not unprocessed_jobs:
+            logger.info("No unprocessed jobs found")
+            return []
+
+        results = []
+
+        for i in range(0, len(unprocessed_jobs), self.batch_size):
+            batch = unprocessed_jobs[i:i + self.batch_size]
+            logger.info(
+                f"Processing batch {i // self.batch_size + 1}, jobs {i + 1}-{min(i + self.batch_size, len(unprocessed_jobs))}")
+
+            batch_results = []
+            for job_id, full_link, content in batch:
+                try:
+                    result = self.extract_and_store(job_id, full_link, content)
+                    batch_results.append(result)
+                except Exception as e:
+                    logger.error(f"Failed to process job {job_id}: {e}")
+                    batch_results.append((JobExtraction(), EducationExtraction(requirements=[], raw_text_analyzed="")))
+
+            results.extend(batch_results)
+
+            # Small delay between batches to avoid rate limiting
+            if i + self.batch_size < len(unprocessed_jobs):
+                logger.info("Sleeping 2 seconds between batches...")
+                import time
+                time.sleep(2)
+
+        logger.info(f"Completed processing {len(results)} jobs")
+        return results
+
+    async def extract_and_store_async(self, job_id: int, full_link: str, content: str) -> tuple[
+        JobExtraction, EducationExtraction]:
+        """Async version of extract_and_store"""
+        processed_content = self._preprocess_text(content)
+
+        if not processed_content:
+            logger.warning(f"Job {job_id}: Empty content after preprocessing")
+            return JobExtraction(), EducationExtraction(requirements=[], raw_text_analyzed="")
+
+        retry_count = 0
+        while retry_count < self.max_retries:
+            try:
+                # Extract job information asynchronously
+                job_data = await self.job_chain.ainvoke({
+                    "text": processed_content,
+                    "format_instructions": self.job_parser.get_format_instructions()
+                })
+                job_data.full_link = full_link
+                job_data.raw_text_analyzed = content[:1000]
+
+                # Extract education requirements asynchronously
+                education_data = await self.education_chain.ainvoke({
+                    "text": processed_content,
+                    "format_instructions": self.education_parser.get_format_instructions()
+                })
+                education_data.raw_text_analyzed = content[:500]
+
+                # Store in database (synchronous)
+                self._store_job_data(job_id, job_data, education_data)
+
+                logger.info(f"[async] Job {job_id}: Successfully processed")
+                return job_data, education_data
+
+            except Exception as e:
+                retry_count += 1
+                logger.error(f"[async] Job {job_id}: Attempt {retry_count} failed: {e}")
+
+                if retry_count >= self.max_retries:
+                    logger.error(f"[async] Job {job_id}: Max retries exceeded")
+                    empty_job = JobExtraction(full_link=full_link, raw_text_analyzed=content[:1000] if content else "")
+                    empty_education = EducationExtraction(requirements=[],
+                                                          raw_text_analyzed=content[:500] if content else "")
+                    return empty_job, empty_education
+
+                await asyncio.sleep(2 ** retry_count)
+
+    async def batch_extract_async(self, max_concurrent: int = 5) -> List[tuple[JobExtraction, EducationExtraction]]:
+        """Process jobs asynchronously with concurrency control"""
+        unprocessed_jobs = self.get_unprocessed_jobs()
+
+        if not unprocessed_jobs:
+            logger.info("No unprocessed jobs found")
+            return []
+
+        # Create semaphore to limit concurrent processing
+        semaphore = asyncio.Semaphore(max_concurrent)
+
+        async def process_with_semaphore(job_data):
+            async with semaphore:
+                job_id, full_link, content = job_data
+                return await self.extract_and_store_async(job_id, full_link, content)
+
+        # Process all jobs concurrently but with limit
+        logger.info(f"Starting async processing of {len(unprocessed_jobs)} jobs with max {max_concurrent} concurrent")
+        tasks = [process_with_semaphore(job_data) for job_data in unprocessed_jobs]
+        results = await asyncio.gather(*tasks, return_exceptions=True)
+
+        # Handle any exceptions in results
+        successful_results = []
+        for i, result in enumerate(results):
+            if isinstance(result, Exception):
+                job_id = unprocessed_jobs[i][0]
+                logger.error(f"[async] Job {job_id} failed with exception: {result}")
+                successful_results.append((JobExtraction(), EducationExtraction(requirements=[], raw_text_analyzed="")))
+            else:
+                successful_results.append(result)
+
+        logger.info(f"Completed async processing of {len(successful_results)} jobs")
+        return successful_results
+
+    def get_processing_statistics(self) -> Dict[str, Any]:
+        """Get statistics about the processing pipeline"""
+        conn = sqlite3.connect(self.output_db_path)
+
+        try:
+            stats = {}
+
+            # Total jobs processed
+            stats['total_processed'] = conn.execute("SELECT COUNT(*) FROM jobs_meta").fetchone()[0]
+
+            # Processing status breakdown
+            status_counts = conn.execute("""
+                                         SELECT status, COUNT(*) as count
+                                         FROM processing_status
+                                         GROUP BY status
+                                         """).fetchall()
+            stats['status_breakdown'] = {status: count for status, count in status_counts}
+
+            # Top industries
+            industry_counts = conn.execute("""
+                                           SELECT industry, COUNT(*) as count
+                                           FROM jobs_meta
+                                           WHERE industry IS NOT NULL
+                                           GROUP BY industry
+                                           ORDER BY count DESC
+                                               LIMIT 10
+                                           """).fetchall()
+            stats['top_industries'] = {industry: count for industry, count in industry_counts}
+
+            # Education level distribution
+            edu_counts = conn.execute("""
+                                      SELECT level, COUNT(*) as count
+                                      FROM education_requirements
+                                      GROUP BY level
+                                      ORDER BY count DESC
+                                      """).fetchall()
+            stats['education_levels'] = {level: count for level, count in edu_counts}
+
+            # Salary statistics
+            salary_stats = conn.execute("""
+                                        SELECT AVG(salary_min) as avg_min,
+                                               AVG(salary_max) as avg_max,
+                                               MIN(salary_min) as min_min,
+                                               MAX(salary_max) as max_max,
+                                               COUNT(*)        as count_with_salary
+                                        FROM compensation
+                                        WHERE salary_min IS NOT NULL
+                                           OR salary_max IS NOT NULL
+                                        """).fetchone()
+
+            if salary_stats and salary_stats[4] > 0:  # count_with_salary > 0
+                stats['salary_statistics'] = {
+                    'average_min': round(salary_stats[0], 2) if salary_stats[0] else None,
+                    'average_max': round(salary_stats[1], 2) if salary_stats[1] else None,
+                    'minimum_salary': salary_stats[2],
+                    'maximum_salary': salary_stats[3],
+                    'jobs_with_salary': salary_stats[4]
+                }
+
+            # Remote work statistics
+            work_type_stats = conn.execute("""
+                                           SELECT SUM(CASE WHEN remote = 1 THEN 1 ELSE 0 END) as remote_jobs,
+                                                  SUM(CASE WHEN onsite = 1 THEN 1 ELSE 0 END) as onsite_jobs,
+                                                  SUM(CASE WHEN hybrid = 1 THEN 1 ELSE 0 END) as hybrid_jobs,
+                                                  COUNT(*)                                    as total_jobs
+                                           FROM location_work
+                                           """).fetchone()
+
+            if work_type_stats:
+                stats['work_arrangement'] = {
+                    'remote': work_type_stats[0],
+                    'onsite': work_type_stats[1],
+                    'hybrid': work_type_stats[2],
+                    'total': work_type_stats[3]
+                }
+
+            return stats
+
+        except Exception as e:
+            logger.error(f"Error getting statistics: {e}")
+            return {}
+        finally:
+            conn.close()
+
+    def export_career_insights(self, output_file: str = "career_insights.json") -> bool:
+        """Export career insights for the advising tool"""
+        try:
+            conn = sqlite3.connect(self.output_db_path)
+
+            # Get comprehensive career data
+            career_data = {
+                'job_titles_by_industry': {},
+                'skills_by_industry': {},
+                'education_pathways': {},
+                'salary_ranges_by_level': {},
+                'career_progression_paths': {},
+                'in_demand_skills': {},
+                'certification_recommendations': {},
+                'location_opportunities': {}
+            }
+
+            # Job titles by industry
+            titles_by_industry = conn.execute("""
+                                              SELECT jm.industry, jm.title_clean, COUNT(*) as frequency
+                                              FROM jobs_meta jm
+                                              WHERE jm.industry IS NOT NULL
+                                                AND jm.title_clean IS NOT NULL
+                                              GROUP BY jm.industry, jm.title_clean
+                                              ORDER BY jm.industry, frequency DESC
+                                              """).fetchall()
+
+            for industry, title, freq in titles_by_industry:
+                if industry not in career_data['job_titles_by_industry']:
+                    career_data['job_titles_by_industry'][industry] = []
+                career_data['job_titles_by_industry'][industry].append({
+                    'title': title,
+                    'frequency': freq
+                })
+
+            # Skills by industry
+            skills_query = conn.execute("""
+                                        SELECT jm.industry, st.technical_skills, st.programming_languages, st.frameworks
+                                        FROM jobs_meta jm
+                                                 JOIN skills_taxonomy st ON jm.job_id = st.job_id
+                                        WHERE jm.industry IS NOT NULL
+                                        """).fetchall()
+
+            for industry, tech_skills, prog_langs, frameworks in skills_query:
+                if industry not in career_data['skills_by_industry']:
+                    career_data['skills_by_industry'][industry] = {
+                        'technical_skills': [],
+                        'programming_languages': [],
+                        'frameworks': []
+                    }
+
+                # Parse JSON skills
+                try:
+                    if tech_skills:
+                        career_data['skills_by_industry'][industry]['technical_skills'].extend(json.loads(tech_skills))
+                    if prog_langs:
+                        career_data['skills_by_industry'][industry]['programming_languages'].extend(
+                            json.loads(prog_langs))
+                    if frameworks:
+                        career_data['skills_by_industry'][industry]['frameworks'].extend(json.loads(frameworks))
+                except json.JSONDecodeError:
+                    continue
+
+            # Education pathways
+            edu_pathways = conn.execute("""
+                                        SELECT er.level, er.field, jm.industry, COUNT(*) as frequency
+                                        FROM education_requirements er
+                                                 JOIN jobs_meta jm ON er.job_id = jm.job_id
+                                        WHERE er.field IS NOT NULL
+                                          AND jm.industry IS NOT NULL
+                                        GROUP BY er.level, er.field, jm.industry
+                                        ORDER BY frequency DESC
+                                        """).fetchall()
+
+            for level, field, industry, freq in edu_pathways:
+                pathway_key = f"{level}_{field}"
+                if pathway_key not in career_data['education_pathways']:
+                    career_data['education_pathways'][pathway_key] = {
+                        'education_level': level,
+                        'field_of_study': field,
+                        'industries': []
+                    }
+                career_data['education_pathways'][pathway_key]['industries'].append({
+                    'industry': industry,
+                    'job_count': freq
+                })
+
+            # Salary ranges by level
+            salary_by_level = conn.execute("""
+                                           SELECT jc.level,
+                                                  AVG(c.salary_min) as avg_min,
+                                                  AVG(c.salary_max) as avg_max,
+                                                  COUNT(*) as count
+                                           FROM job_classification jc
+                                               JOIN compensation c
+                                           ON jc.job_id = c.job_id
+                                           WHERE jc.level IS NOT NULL
+                                             AND (c.salary_min IS NOT NULL
+                                              OR c.salary_max IS NOT NULL)
+                                           GROUP BY jc.level
+                                           """).fetchall()
 
-# Persist structured results
-def persist_structured(records: List[Tuple[int, str]]):
-    conn = sqlite3.connect(STRUCT_DB)
-    cur  = conn.cursor()
-    cur.executemany(
-        f"INSERT OR REPLACE INTO {STRUCT_TABLE} (id, json) VALUES (?,?)",
-        records
-    )
-    conn.commit()
-    conn.close()
-    logging.info(f"Saved {len(records)} structured records.")
-
-# Setup LangChain with format instructions
-def create_chain():
-    # Initialize Pydantic parser for schema enforcement
-    parser = PydanticOutputParser(pydantic_object=JobStructured)
-
-    # Base prompt template with placeholder for format instructions
-    prompt_template = PromptTemplate(
-        input_variables=["description", "format_instructions"],
-        template="""
-You are an expert at extracting structured JSON from job description text.
-Return exactly the JSON matching the JobStructured schema.
-
-Job Description:
-{description}
-
-{
-  "id": int,
-  "company_name": string,
-  "location": string,
-  "post_date": string (YYYY-MM-DD or empty),
-  "education_requirements": [
-    {
-      "level": "diploma|certificate|associate|bachelor|master|phd|professional_license|none_specified",
-      "field": string,
-      "requirement_type": "required|preferred|equivalent_experience_accepted",
-      "years_experience_in_lieu": number or null
-    }
-  ],
-  "job_classification": { /* follow JobClassification model */ },
-  "location_and_work": { /* follow LocationWork model */ },
-  "skills": { /* follow SkillsTaxonomy model */ },
-  "certifications": [ /* list of Certification objects */ ],
-  "career_progression": { /* follow CareerProgression model */ },
-  "compensation": { /* follow CompensationBenefits model */ },
-  "market_intelligence": { /* follow MarketIntelligence model */ },
-  "work_environment": { /* follow WorkEnvironment model */ }
-}
+            for level, avg_min, avg_max, count in salary_by_level:
+                career_data['salary_ranges_by_level'][level] = {
+                    'average_min_salary': round(avg_min, 2) if avg_min else None,
+                    'average_max_salary': round(avg_max, 2) if avg_max else None,
+                    'sample_size': count
+                }
+
+            # Career progression paths
+            progression_data = conn.execute("""
+                                            SELECT entry_level, mid_level, senior_level, COUNT(*) as frequency
+                                            FROM career_progression
+                                            WHERE entry_level IS NOT NULL
+                                               OR mid_level IS NOT NULL
+                                               OR senior_level IS NOT NULL
+                                            GROUP BY entry_level, mid_level, senior_level
+                                            ORDER BY frequency DESC
+                                            """).fetchall()
+
+            for entry, mid, senior, freq in progression_data:
+                if any([entry, mid, senior]):
+                    path_key = f"{entry or 'Unknown'}_to_{senior or 'Unknown'}"
+                    career_data['career_progression_paths'][path_key] = {
+                        'entry_level': entry,
+                        'mid_level': mid,
+                        'senior_level': senior,
+                        'frequency': freq
+                    }
+
+            # In-demand skills (most frequently mentioned)
+            all_skills = []
+            skills_data = conn.execute("""
+                                       SELECT technical_skills, programming_languages, frameworks
+                                       FROM skills_taxonomy
+                                       """).fetchall()
+
+            for tech_skills, prog_langs, frameworks in skills_data:
+                try:
+                    if tech_skills:
+                        all_skills.extend(json.loads(tech_skills))
+                    if prog_langs:
+                        all_skills.extend(json.loads(prog_langs))
+                    if frameworks:
+                        all_skills.extend(json.loads(frameworks))
+                except json.JSONDecodeError:
+                    continue
+
+            # Count skill frequencies
+            from collections import Counter
+            skill_counts = Counter(all_skills)
+            career_data['in_demand_skills'] = dict(skill_counts.most_common(50))
+
+            # Certification recommendations
+            cert_data = conn.execute("""
+                                     SELECT c.name,
+                                            c.issuer,
+                                            COUNT(*)                                        as frequency,
+                                            AVG(CASE WHEN c.required = 1 THEN 1 ELSE 0 END) as required_ratio
+                                     FROM certifications c
+                                     WHERE c.name IS NOT NULL
+                                     GROUP BY c.name, c.issuer
+                                     ORDER BY frequency DESC LIMIT 20
+                                     """).fetchall()
+
+            for name, issuer, freq, req_ratio in cert_data:
+                career_data['certification_recommendations'][name] = {
+                    'issuer': issuer,
+                    'frequency': freq,
+                    'often_required': req_ratio > 0.5
+                }
+
+            # Location opportunities
+            location_data = conn.execute("""
+                                         SELECT lw.office_location,
+                                                COUNT(*)          as job_count,
+                                                AVG(c.salary_min) as avg_salary_min,
+                                                AVG(c.salary_max) as avg_salary_max
+                                         FROM location_work lw
+                                                  LEFT JOIN compensation c ON lw.job_id = c.job_id
+                                         WHERE lw.office_location IS NOT NULL
+                                         GROUP BY lw.office_location
+                                         ORDER BY job_count DESC LIMIT 30
+                                         """).fetchall()
+
+            for location, job_count, avg_min, avg_max in location_data:
+                career_data['location_opportunities'][location] = {
+                    'job_count': job_count,
+                    'average_salary_min': round(avg_min, 2) if avg_min else None,
+                    'average_salary_max': round(avg_max, 2) if avg_max else None
+                }
+
+            # Add metadata
+            career_data['metadata'] = {
+                'generated_at': datetime.now().isoformat(),
+                'total_jobs_analyzed': conn.execute("SELECT COUNT(*) FROM jobs_meta").fetchone()[0],
+                'data_sources': ['job_postings'],
+                'version': '1.0'
+            }
+
+            # Write to file
+            with open(output_file, 'w', encoding='utf-8') as f:
+                json.dump(career_data, f, indent=2, ensure_ascii=False)
+
+            logger.info(f"Career insights exported to {output_file}")
+            return True
+
+        except Exception as e:
+            logger.error(f"Error exporting career insights: {e}")
+            return False
+        finally:
+            conn.close()
+
+    def cleanup_failed_jobs(self) -> int:
+        """Clean up jobs that have repeatedly failed processing"""
+        conn = sqlite3.connect(self.output_db_path)
+
+        try:
+            # Delete jobs that have failed more than max_retries times
+            cursor = conn.execute("""
+                                  DELETE
+                                  FROM processing_status
+                                  WHERE status = 'error'
+                                    AND retry_count > ?
+                                  """, (self.max_retries,))
+
+            deleted_count = cursor.rowcount
+            conn.commit()
+
+            if deleted_count > 0:
+                logger.info(f"Cleaned up {deleted_count} failed job records")
+
+            return deleted_count
+
+        except Exception as e:
+            logger.error(f"Error during cleanup: {e}")
+            return 0
+        finally:
+            conn.close()
 
-{format_instructions}
-"""
-    )
+    def validate_database_integrity(self) -> Dict[str, Any]:
+        """Validate the integrity of the processed data"""
+        conn = sqlite3.connect(self.output_db_path)
 
-    # Inject format instructions so chain only needs 'description'
-    prompt = prompt_template.partial(
-        format_instructions=parser.get_format_instructions()
-    )
-
-    llm = OpenAI(temperature=0)
-    chain = LLMChain(
-        llm=llm,
-        prompt=prompt
-    )
-    return chain, parser
-
-# Process a single record with validation with validation
-def process_record(record: Tuple[int, str], chain: LLMChain, parser: PydanticOutputParser) -> Optional[Tuple[int, str]]:
-    job_id, text = record
-    try:
-        response = chain.invoke({"description": text})
-        # Parse and validate
-        structured: JobStructured = parser.parse(response)
-        data = structured.dict()
-        return (job_id, json.dumps(data))
-    except OutputParserException as e:
-        logging.error(f"Parsing failed id={job_id}: {e}")
+        try:
+            validation_results = {
+                'issues': [],
+                'warnings': [],
+                'summary': {}
+            }
+
+            # Check for orphaned records
+            orphaned_checks = [
+                ("job_classification", "jobs_meta"),
+                ("location_work", "jobs_meta"),
+                ("skills_taxonomy", "jobs_meta"),
+                ("certifications", "jobs_meta"),
+                ("career_progression", "jobs_meta"),
+                ("compensation", "jobs_meta"),
+                ("education_requirements", "jobs_meta")
+            ]
+
+            for child_table, parent_table in orphaned_checks:
+                orphaned = conn.execute(f"""
+                    SELECT COUNT(*) FROM {child_table} c
+                    LEFT JOIN {parent_table} p ON c.job_id = p.job_id
+                    WHERE p.job_id IS NULL
+                """).fetchone()[0]
+
+                if orphaned > 0:
+                    validation_results['issues'].append(f"Found {orphaned} orphaned records in {child_table}")
+
+            # Check for missing critical data
+            missing_titles = \
+            conn.execute("SELECT COUNT(*) FROM jobs_meta WHERE title_clean IS NULL OR title_clean = ''").fetchone()[0]
+            if missing_titles > 0:
+                validation_results['warnings'].append(f"{missing_titles} jobs missing clean titles")
+
+            missing_companies = \
+            conn.execute("SELECT COUNT(*) FROM jobs_meta WHERE company IS NULL OR company = ''").fetchone()[0]
+            if missing_companies > 0:
+                validation_results['warnings'].append(f"{missing_companies} jobs missing company information")
+
+            # Check data quality scores
+            low_confidence_edu = \
+            conn.execute("SELECT COUNT(*) FROM education_requirements WHERE confidence_score < 0.5").fetchone()[0]
+            if low_confidence_edu > 0:
+                validation_results['warnings'].append(
+                    f"{low_confidence_edu} education requirements with low confidence scores")
+
+            # Summary statistics
+            validation_results['summary'] = {
+                'total_jobs': conn.execute("SELECT COUNT(*) FROM jobs_meta").fetchone()[0],
+                'total_education_requirements': conn.execute("SELECT COUNT(*) FROM education_requirements").fetchone()[
+                    0],
+                'total_certifications': conn.execute("SELECT COUNT(*) FROM certifications").fetchone()[0],
+                'jobs_with_salary_info': conn.execute(
+                    "SELECT COUNT(*) FROM compensation WHERE salary_min IS NOT NULL OR salary_max IS NOT NULL").fetchone()[
+                    0]
+            }
+
+            return validation_results
+
+        except Exception as e:
+            logger.error(f"Error during validation: {e}")
+            return {'error': str(e)}
+        finally:
+            conn.close()
+
+
+def main():
+    """Main function to run the processing pipeline"""
+    try:
+        # Initialize processor
+        processor = AcademicDetailsProcessor(
+            input_db_path="db/jobs.sqlite3",
+            output_db_path="db/processed_jobs.sqlite3",
+            batch_size=10,
+            max_retries=3
+        )
+
+        logger.info("Starting job processing pipeline...")
+
+        # Run synchronous batch processing
+        results = processor.batch_extract()
+
+        # Get and log statistics
+        stats = processor.get_processing_statistics()
+        logger.info(f"Processing complete. Statistics: {stats}")
+
+        # Export career insights
+        success = processor.export_career_insights("career_insights.json")
+        if success:
+            logger.info("Career insights exported successfully")
+
+        # Validate data integrity
+        validation = processor.validate_database_integrity()
+        if validation.get('issues'):
+            logger.warning(f"Data integrity issues found: {validation['issues']}")
+
+        # Cleanup failed jobs
+        cleaned = processor.cleanup_failed_jobs()
+        if cleaned > 0:
+            logger.info(f"Cleaned up {cleaned} failed job records")
+
+        logger.info("Pipeline execution completed successfully")
+
     except Exception as e:
-        logging.error(f"Failed to process id={job_id}: {e}")
-    return None
+        logger.error(f"Pipeline execution failed: {e}")
+        raise
+
+
+async def main_async():
+    """Async version of main function"""
+    try:
+        processor = AcademicDetailsProcessor(
+            input_db_path="db/jobs.sqlite3",
+            output_db_path="db/processed_jobs.sqlite3",
+            batch_size=10,
+            max_retries=3
+        )
+
+        logger.info("Starting async job processing pipeline...")
+
+        # Run asynchronous batch processing
+        results = await processor.batch_extract_async(max_concurrent=5)
+
+        # Get and log statistics
+        stats = processor.get_processing_statistics()
+        logger.info(f"Async processing complete. Statistics: {stats}")
 
-# Main batch processing
-def main(batch: int = 10, workers: int = 5):
-    ensure_structured_db()
-    raw_records = fetch_raw(batch)
-    if not raw_records:
-        logging.info("No new records to process.")
-        return
+        # Export career insights
+        success = processor.export_career_insights("career_insights_async.json")
+        if success:
+            logger.info("Career insights exported successfully")
 
-    chain, parser = create_chain()
-    results = []
-    with ThreadPoolExecutor(max_workers=workers) as executor:
-        futures = [executor.submit(process_record, r, chain, parser) for r in raw_records]
-        for fut in as_completed(futures):
-            res = fut.result()
-            if res:
-                results.append(res)
+        logger.info("Async pipeline execution completed successfully")
 
-    persist_structured(results)
+    except Exception as e:
+        logger.error(f"Async pipeline execution failed: {e}")
+        raise
+
 
 if __name__ == "__main__":
-    import argparse
-    parser = argparse.ArgumentParser(description="Process raw job descriptions to structured JSON.")
-    parser.add_argument("--batch", type=int, default=10)
-    parser.add_argument("--workers", type=int, default=5)
-    args = parser.parse_args()
-    main(batch=args.batch, workers=args.workers)
+    import sys
+
+    if len(sys.argv) > 1 and sys.argv[1] == "--async":
+        asyncio.run(main_async())
+    else:
+        main()
\ No newline at end of file
Index: .idea/vcs.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project version=\"4\">\n  <component name=\"VcsDirectoryMappings\">\n    <mapping directory=\"$PROJECT_DIR$/Nextstepjobs\" vcs=\"Git\" />\n  </component>\n</project>
===================================================================
diff --git a/.idea/vcs.xml b/.idea/vcs.xml
--- a/.idea/vcs.xml	(revision 149515dafb2fe9d1a83f1abe1d94719b1f8c707a)
+++ b/.idea/vcs.xml	(date 1751306667549)
@@ -1,6 +1,6 @@
 <?xml version="1.0" encoding="UTF-8"?>
 <project version="4">
   <component name="VcsDirectoryMappings">
-    <mapping directory="$PROJECT_DIR$/Nextstepjobs" vcs="Git" />
+    <mapping directory="$PROJECT_DIR$" vcs="Git" />
   </component>
 </project>
\ No newline at end of file
Index: .gitignore
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>.DS_Store\n.idea/\n.DS_Store\ndb/jobs.sqlite3\n
===================================================================
diff --git a/.gitignore b/.gitignore
--- a/.gitignore	(revision 149515dafb2fe9d1a83f1abe1d94719b1f8c707a)
+++ b/.gitignore	(date 1751306692828)
@@ -2,3 +2,8 @@
 .idea/
 .DS_Store
 db/jobs.sqlite3
+db/*.sqlite3
+*.sqlite3
+*.db
+.venv
+.env
Index: processors/pipeline2.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import os\nimport re\nimport sqlite3\nimport logging\nimport asyncio\nfrom typing import List, Optional, Literal\n\nfrom dotenv import load_dotenv\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_openai import OpenAI\nfrom langchain_core.output_parsers import PydanticOutputParser\nfrom langchain_core.runnables import RunnableSequence\nfrom pydantic import BaseModel, Field\n\n# Load environment variables\nload_dotenv()\n\n# Regex patterns for abbreviation normalization\nBS_RE = re.compile(r\"\\bB\\.S\\.\\b\", re.IGNORECASE)\nBA_RE = re.compile(r\"\\bB\\.A\\.\\b\", re.IGNORECASE)\nMS_RE = re.compile(r\"\\bM\\.S\\.\\b\", re.IGNORECASE)\nMA_RE = re.compile(r\"\\bM\\.A\\.\\b\", re.IGNORECASE)\nPHD_RE = re.compile(r\"\\bPh\\.D\\.\\b\", re.IGNORECASE)\nWHITESPACE_RE = re.compile(r\"\\s+\\n?\")\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Data models with defaults to handle missing fields\nclass JobClassification(BaseModel):\n    category: Optional[str] = None\n    level: Optional[str] = None\n    function: Optional[str] = None\n\nclass LocationWork(BaseModel):\n    office_location: Optional[str] = None\n    remote: Optional[bool] = None\n    onsite: Optional[bool] = None\n\nclass SkillsTaxonomy(BaseModel):\n    main_skill: Optional[str] = None\n    specific_skills: List[str] = Field(default_factory=list)\n    desirable_skills: List[str] = Field(default_factory=list)\n\nclass Certification(BaseModel):\n    name: Optional[str] = None\n    issuer: Optional[str] = None\n    year: Optional[int] = None\n\nclass CareerProgression(BaseModel):\n    entry_level: Optional[str] = None\n    mid_level: Optional[str] = None\n    senior_level: Optional[str] = None\n\nclass CompensationBenefits(BaseModel):\n    salary_min: Optional[float] = None\n    salary_max: Optional[float] = None\n    benefits: List[str] = Field(default_factory=list)\n\nclass JobExtraction(BaseModel):\n    # Basic fields\n    title_clean: Optional[str] = None\n    company: Optional[str] = None\n    company_location: Optional[str] = None\n    post_date: Optional[str] = None\n    industry: Optional[str] = None\n    job_type: Optional[str] = None\n    job_category: Optional[str] = None\n    job_description: Optional[str] = None\n    application_deadline: Optional[str] = None\n    additional_requirements: Optional[str] = None\n    full_link: Optional[str] = None\n\n    # Nested structures with defaults\n    job_classification: JobClassification = Field(default_factory=JobClassification)\n    location_and_work: LocationWork = Field(default_factory=LocationWork)\n    skills: SkillsTaxonomy = Field(default_factory=SkillsTaxonomy)\n    certifications: List[Certification] = Field(default_factory=list)\n    career_progression: CareerProgression = Field(default_factory=CareerProgression)\n    compensation: CompensationBenefits = Field(default_factory=CompensationBenefits)\n\n    raw_text_analyzed: str = Field(default=\"\")\n\nclass AcademicDetailsProcessor:\n    def __init__(\n        self,\n        input_db_path: str = \"db/jobs.sqlite3\",\n        output_db_path: str = \"db/processed_jobs.sqlite3\",\n        llm_model: str = \"gpt-4o-mini\",\n        temperature: float = 0.1,\n        api_key: Optional[str] = None\n    ):\n        key = api_key or os.getenv(\"OPENAI_API_KEY\")\n        if not key:\n            raise ValueError(\"OpenAI API key must be set via parameter or env var\")\n        self.llm = OpenAI(model=llm_model, temperature=temperature, openai_api_key=key)\n        self.parser = PydanticOutputParser(pydantic_object=JobExtraction)\n        self.chain: RunnableSequence = self._create_chain()\n\n        self.input_db_path = input_db_path\n        self.output_db_path = output_db_path\n        self._setup_db()\n\n    def _setup_db(self):\n        conn = sqlite3.connect(self.output_db_path)\n        # metadata table\n        conn.execute(\n            \"\"\"\n            CREATE TABLE IF NOT EXISTS jobs_meta (\n                job_id INTEGER PRIMARY KEY,\n                full_link TEXT,\n                title_clean TEXT,\n                company TEXT,\n                company_location TEXT,\n                post_date TEXT,\n                industry TEXT,\n                job_type TEXT,\n                job_category TEXT,\n                job_description TEXT,\n                application_deadline TEXT,\n                additional_requirements TEXT\n            );\n            \"\"\"\n        )\n        conn.execute(\"CREATE INDEX IF NOT EXISTS idx_meta_job ON jobs_meta(job_id);\")\n        # nested tables\n        conn.execute(\n            \"\"\"\n            CREATE TABLE IF NOT EXISTS job_classification (\n                job_id INTEGER PRIMARY KEY,\n                category TEXT,\n                level TEXT,\n                function TEXT,\n                FOREIGN KEY(job_id) REFERENCES jobs_meta(job_id)\n            );\n            \"\"\"\n        )\n        conn.execute(\n            \"\"\"\n            CREATE TABLE IF NOT EXISTS location_work (\n                job_id INTEGER PRIMARY KEY,\n                office_location TEXT,\n                remote BOOLEAN,\n                onsite BOOLEAN,\n                FOREIGN KEY(job_id) REFERENCES jobs_meta(job_id)\n            );\n            \"\"\"\n        )\n        conn.execute(\n            \"\"\"\n            CREATE TABLE IF NOT EXISTS skills_taxonomy (\n                job_id INTEGER PRIMARY KEY,\n                main_skill TEXT,\n                specific_skills TEXT,\n                desirable_skills TEXT,\n                FOREIGN KEY(job_id) REFERENCES jobs_meta(job_id)\n            );\n            \"\"\"\n        )\n        conn.execute(\n            \"\"\"\n            CREATE TABLE IF NOT EXISTS certifications (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                job_id INTEGER,\n                name TEXT,\n                issuer TEXT,\n                year INTEGER,\n                FOREIGN KEY(job_id) REFERENCES jobs_meta(job_id)\n            );\n            \"\"\"\n        )\n        conn.execute(\n            \"\"\"\n            CREATE TABLE IF NOT EXISTS career_progression (\n                job_id INTEGER PRIMARY KEY,\n                entry_level TEXT,\n                mid_level TEXT,\n                senior_level TEXT,\n                FOREIGN KEY(job_id) REFERENCES jobs_meta(job_id)\n            );\n            \"\"\"\n        )\n        conn.execute(\n            \"\"\"\n            CREATE TABLE IF NOT EXISTS compensation (\n                job_id INTEGER PRIMARY KEY,\n                salary_min REAL,\n                salary_max REAL,\n                benefits TEXT,\n                FOREIGN KEY(job_id) REFERENCES jobs_meta(job_id)\n            );\n            \"\"\"\n        )\n        conn.commit()\n        conn.close()\n\n    def _create_chain(self) -> RunnableSequence:\n        prompt = PromptTemplate.from_template(\n            \"Extract the following from the job posting text:\\n\"\n            \"- Full link, clean title, company, company location, post date (YYYY-MM-DD)\\n\"\n            \"- Industry, job type, category, description, application deadline, additional requirements\\n\"\n            \"- Nested: job_classification, location_and_work, skills (main & specific & desirable), certifications, career_progression, compensation\\n\\n\"\n            \"TEXT:\\n{text}\\n\\n{format_instructions}\"\n        )\n        return prompt | self.llm | self.parser\n\n    def _preprocess_text(self, text: str) -> str:\n        t = WHITESPACE_RE.sub(\" \", text)\n        for rex, sub in [(BS_RE, \"Bachelor\"), (BA_RE, \"Bachelor\"), (MS_RE, \"Master\"), (MA_RE, \"Master\"), (PHD_RE, \"PhD\")]:\n            t = rex.sub(sub, t)\n        return t.strip()\n\n    def extract_and_store(self, job_id: int, full_link: str, content: str) -> JobExtraction:\n        processed = self._preprocess_text(content)\n        try:\n            data: JobExtraction = self.chain.invoke({\n                \"text\": processed,\n                \"format_instructions\": self.parser.get_format_instructions()\n            })\n            data.raw_text_analyzed = content[:500]\n            logger.info(f\"Job {job_id}: extracted fields\")\n            conn = sqlite3.connect(self.output_db_path)\n            try:\n                conn.execute(\"BEGIN\")\n                # metadata\n                conn.execute(\n                    \"INSERT OR REPLACE INTO jobs_meta VALUES (?,?,?,?,?,?,?,?,?,?,?,?);\",\n                    (job_id, full_link, data.title_clean, data.company, data.company_location,\n                     data.post_date, data.industry, data.job_type, data.job_category,\n                     data.job_description, data.application_deadline, data.additional_requirements)\n                )\n                # classification\n                conn.execute(\n                    \"INSERT OR REPLACE INTO job_classification VALUES (?,?,?,?);\",\n                    (job_id, data.job_classification.category, data.job_classification.level, data.job_classification.function)\n                )\n                # location work\n                conn.execute(\n                    \"INSERT OR REPLACE INTO location_work VALUES (?,?,?,?);\",\n                    (job_id, data.location_and_work.office_location, data.location_and_work.remote, data.location_and_work.onsite)\n                )\n                # skills taxonomy\n                import json\n                conn.execute(\n                    \"INSERT OR REPLACE INTO skills_taxonomy VALUES (?,?,?,?);\",\n                    (job_id, data.skills.main_skill, json.dumps(data.skills.specific_skills), json.dumps(data.skills.desirable_skills))\n                )\n                # certifications\n                for cert in data.certifications:\n                    conn.execute(\n                        \"INSERT INTO certifications (job_id,name,issuer,year) VALUES (?,?,?,?);\",\n                        (job_id, cert.name, cert.issuer, cert.year)\n                    )\n                # career progression\n                conn.execute(\n                    \"INSERT OR REPLACE INTO career_progression VALUES (?,?,?,?);\",\n                    (job_id, data.career_progression.entry_level, data.career_progression.mid_level, data.career_progression.senior_level)\n                )\n                # compensation\n                conn.execute(\n                    \"INSERT OR REPLACE INTO compensation VALUES (?,?,?,?);\",\n                    (job_id, data.compensation.salary_min, data.compensation.salary_max, json.dumps(data.compensation.benefits))\n                )\n                conn.commit()\n            except Exception as db_e:\n                conn.rollback()\n                logger.error(f\"Job {job_id}: DB failed: {db_e}\")\n                raise\n            finally:\n                conn.close()\n            return data\n        except Exception as e:\n            logger.error(f\"Job {job_id}: extraction failed: {e}\")\n            return JobExtraction(raw_text_analyzed=content)\n\n    def batch_extract(self) -> List[JobExtraction]:\n        conn = sqlite3.connect(self.input_db_path)\n        rows = conn.execute(\"SELECT id, full_link, content FROM jobs_data\").fetchall()\n        conn.close()\n        return [self.extract_and_store(jid, link, txt) for jid, link, txt in rows]\n\n    async def batch_extract_async(self) -> List[JobExtraction]:\n        conn = sqlite3.connect(self.input_db_path)\n        rows = conn.execute(\"SELECT id, full_link, content FROM jobs_data\").fetchall()\n        conn.close()\n        tasks = [self.extract_and_store(jid, link, txt) for jid, link, txt in rows]\n        return await asyncio.gather(*tasks)\n\nif __name__ == \"__main__\":\n    processor = AcademicDetailsProcessor()\n    processor.batch_extract()\n    # async_results = asyncio.run(processor.batch_extract_async())\n
===================================================================
diff --git a/processors/pipeline2.py b/processors/pipeline2.py
--- a/processors/pipeline2.py	(revision 149515dafb2fe9d1a83f1abe1d94719b1f8c707a)
+++ b/processors/pipeline2.py	(date 1751306692897)
@@ -58,6 +58,21 @@
     salary_max: Optional[float] = None
     benefits: List[str] = Field(default_factory=list)
 
+class EducationRequirement(BaseModel):
+    level: Literal[
+        "high_school", "certificate", "diploma", "associate",
+        "bachelor", "master", "phd", "professional_license",
+        "none_specified", "equivalent_experience"
+    ]
+    field: Optional[str]
+    requirement_type: Literal["required", "preferred", "equivalent_experience_accepted"]
+    years_experience_substitute: Optional[int]
+    confidence_score: float
+
+class EducationExtraction(BaseModel):
+    requirements: List[EducationRequirement]
+    raw_text_analyzed: str
+
 class JobExtraction(BaseModel):
     # Basic fields
     title_clean: Optional[str] = None
@@ -91,6 +106,7 @@
         temperature: float = 0.1,
         api_key: Optional[str] = None
     ):
+        self.output_parser = None
         key = api_key or os.getenv("OPENAI_API_KEY")
         if not key:
             raise ValueError("OpenAI API key must be set via parameter or env var")
@@ -192,6 +208,25 @@
             );
             """
         )
+        conn.execute(
+        """
+        CREATE TABLE IF NOT EXISTS education_requirements
+        (
+            id INTEGER PRIMARY KEY AUTOINCREMENT,
+            job_id INTEGER NOT NULL,
+            level TEXT NOT NULL,
+            field TEXT,
+            requirement_type TEXT NOT NULL,
+            years_experience_substitute INTEGER,
+            confidence_score REAL NOT NULL,
+            FOREIGN KEY(job_id) REFERENCES jobs_data(id)
+            );
+        """
+        )
+        # index for faster lookups
+        conn.execute(
+            "CREATE INDEX IF NOT EXISTS idx_edu_job ON education_requirements(job_id);"
+        )
         conn.commit()
         conn.close()
 
@@ -211,6 +246,13 @@
             t = rex.sub(sub, t)
         return t.strip()
 
+    def _post_process_results(self, extraction: EducationExtraction) -> EducationExtraction:
+        for req in extraction.requirements:
+            req.confidence_score = min(max(req.confidence_score, 0.0), 1.0)
+            if req.field:
+                req.field = req.field.lower().strip()
+        return extraction
+
     def extract_and_store(self, job_id: int, full_link: str, content: str) -> JobExtraction:
         processed = self._preprocess_text(content)
         try:
@@ -218,8 +260,13 @@
                 "text": processed,
                 "format_instructions": self.parser.get_format_instructions()
             })
+            result: EducationExtraction = self.chain.invoke({
+                "text": processed,
+                "format_instructions": self.output_parser.get_format_instructions()
+            })
             data.raw_text_analyzed = content[:500]
             logger.info(f"Job {job_id}: extracted fields")
+            logger.info(f"Job {job_id}: extracted {len(result.requirements)} requirements")
             conn = sqlite3.connect(self.output_db_path)
             try:
                 conn.execute("BEGIN")
@@ -262,6 +309,15 @@
                     "INSERT OR REPLACE INTO compensation VALUES (?,?,?,?);",
                     (job_id, data.compensation.salary_min, data.compensation.salary_max, json.dumps(data.compensation.benefits))
                 )
+                # Education Requirements
+                conn.execute("BEGIN")
+                for req in result.requirements:
+                    conn.execute(
+                        "INSERT INTO education_requirements (job_id, level, field, requirement_type, years_experience_substitute, confidence_score) VALUES (?, ?, ?, ?, ?, ?)",
+                        (job_id, req.level, req.field, req.requirement_type,
+                         req.years_experience_substitute, req.confidence_score)
+                    )
+                conn.commit()
                 conn.commit()
             except Exception as db_e:
                 conn.rollback()
@@ -280,6 +336,38 @@
         conn.close()
         return [self.extract_and_store(jid, link, txt) for jid, link, txt in rows]
 
+    async def extract_and_store_async(self, job_id: int, job_content: str) -> EducationExtraction:
+        processed = self._preprocess_text(job_content)
+        try:
+            result: EducationExtraction = await self.chain.ainvoke({
+                "text": processed,
+                "format_instructions": self.output_parser.get_format_instructions()
+            })
+            result = self._post_process_results(result)
+            logger.info(f"[async] Job {job_id}: extracted {len(result.requirements)} requirements")
+
+            conn = sqlite3.connect(self.output_db_path)
+            try:
+                conn.execute("BEGIN")
+                for req in result.requirements:
+                    conn.execute(
+                        "INSERT INTO education_requirements (job_id, level, field, requirement_type, years_experience_substitute, confidence_score) VALUES (?, ?, ?, ?, ?, ?)",
+                        (job_id, req.level, req.field, req.requirement_type,
+                         req.years_experience_substitute, req.confidence_score)
+                    )
+                conn.commit()
+            except Exception as db_e:
+                conn.rollback()
+                logger.error(f"[async] Job {job_id}: DB transaction failed: {db_e}")
+                raise
+            finally:
+                conn.close()
+
+            return result
+        except Exception as e:
+            logger.error(f"[async] Job {job_id}: extraction failed: {e}")
+            return EducationExtraction(requirements=[], raw_text_analyzed=job_content)
+
     async def batch_extract_async(self) -> List[JobExtraction]:
         conn = sqlite3.connect(self.input_db_path)
         rows = conn.execute("SELECT id, full_link, content FROM jobs_data").fetchall()
